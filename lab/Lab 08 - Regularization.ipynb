{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 08 - Regularization\n",
    "\n",
    "In previous labs we fitted and analyzed different algorithms for different hypothesis classes. In many of those, the chosen hypothesis was the one minimizing some cost function $\\mathcal{F}_S\\left(h\\right)$ for some training set $S=\\left\\{\\left(\\mathbf{x}_i,y_i\\right)\\right\\}^m_{i=1}$, and where $\\mathcal{F}$ measures the goodness of $h$'s fit to $S$. \n",
    "\n",
    "Along side fitting a hypothesis, we have also discussed the richness and expressiveness of different hypothesis classes. For example, we have seen how the depth of a classification tree or the degree of a fitted polynomial influences prediction. We built the intuition of how the richness of the hypothesis class, through which the richness of the selected hypothesis, influences the bias-variance treade-off and the generalization error. \n",
    "\n",
    "The concept of **regularization** is of constraining the fitting process to enable the selection of complex models, but only if it is indeed \"justified enough\". Instead of minimizing only $\\mathcal{F}$ we introduce an additional **regularization term** that depends on the tested hypothesis. We are searching for hypotheses that minimized the joint expression $$ h_S = \\underset{h\\in\\mathcal{H}}{\\text{argmin}} \\,\\, \\mathcal{F}_S\\left(h\\right) + \\lambda \\mathcal{R}\\left(h\\right) $$  \n",
    "where we select $\\mathcal{R}$ in a way that measures the \"complexity\" of the hypothesis $h$. In this lab we focus on two modern regularization terms for regression: Lasso- and Ridge regressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data and Fitting Models\n",
    "To investigate how Lasso and Ridge regressions work we will use the [mtcars dataset](https://www.rdocumentation.org/packages/datasets/versions/3.6.2/topics/mtcars). This dataset was extracted from the 1974 Motor Trend US magazine, and comprises fuel consumption and 10 aspects of automobile design and performance for 32 automobiles (1973-74 models).\n",
    "\n",
    "*The `mpg` column, which stands for miles per gallon of fuel is the response value to be predicted.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>mpg</th>\n",
       "      <th>cyl</th>\n",
       "      <th>disp</th>\n",
       "      <th>hp</th>\n",
       "      <th>drat</th>\n",
       "      <th>wt</th>\n",
       "      <th>qsec</th>\n",
       "      <th>vs</th>\n",
       "      <th>am</th>\n",
       "      <th>gear</th>\n",
       "      <th>carb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mazda RX4</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6</td>\n",
       "      <td>160.0</td>\n",
       "      <td>110</td>\n",
       "      <td>3.90</td>\n",
       "      <td>2.620</td>\n",
       "      <td>16.46</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mazda RX4 Wag</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6</td>\n",
       "      <td>160.0</td>\n",
       "      <td>110</td>\n",
       "      <td>3.90</td>\n",
       "      <td>2.875</td>\n",
       "      <td>17.02</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Datsun 710</td>\n",
       "      <td>22.8</td>\n",
       "      <td>4</td>\n",
       "      <td>108.0</td>\n",
       "      <td>93</td>\n",
       "      <td>3.85</td>\n",
       "      <td>2.320</td>\n",
       "      <td>18.61</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hornet 4 Drive</td>\n",
       "      <td>21.4</td>\n",
       "      <td>6</td>\n",
       "      <td>258.0</td>\n",
       "      <td>110</td>\n",
       "      <td>3.08</td>\n",
       "      <td>3.215</td>\n",
       "      <td>19.44</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hornet Sportabout</td>\n",
       "      <td>18.7</td>\n",
       "      <td>8</td>\n",
       "      <td>360.0</td>\n",
       "      <td>175</td>\n",
       "      <td>3.15</td>\n",
       "      <td>3.440</td>\n",
       "      <td>17.02</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               model   mpg  cyl   disp   hp  drat     wt   qsec  vs  am  gear  \\\n",
       "0          Mazda RX4  21.0    6  160.0  110  3.90  2.620  16.46   0   1     4   \n",
       "1      Mazda RX4 Wag  21.0    6  160.0  110  3.90  2.875  17.02   0   1     4   \n",
       "2         Datsun 710  22.8    4  108.0   93  3.85  2.320  18.61   1   1     4   \n",
       "3     Hornet 4 Drive  21.4    6  258.0  110  3.08  3.215  19.44   1   0     3   \n",
       "4  Hornet Sportabout  18.7    8  360.0  175  3.15  3.440  17.02   0   0     3   \n",
       "\n",
       "   carb  \n",
       "0     4  \n",
       "1     4  \n",
       "2     1  \n",
       "3     1  \n",
       "4     2  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
    "\n",
    "np.random.seed(0)\n",
    "# X = pd.read_csv(\"../datasets/mtcars.csv\").drop(columns=[\"model\"]).dropna()\n",
    "X = pd.read_csv(\"../datasets/mtcars.csv\").dropna()\n",
    "\n",
    "tr, te= train_test_split(X, test_size=0.4)\n",
    "X_train, y_train, X_test, y_test = tr.loc[:, tr.columns != \"mpg\"], tr[\"mpg\"], te.loc[:, te.columns != \"mpg\"], te[\"mpg\"]\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Ridge and Lasso models minimize the joint loss of the $MSE$ and a regularization terms of the coefficient's vector size (norm). In the case of Ridge the norm is the $\\ell_2$ Euclidean norm while in the case of Lasso it is the $\\ell_1$ norm:\n",
    "$$\n",
    "\\hat w^{ridge}_{\\lambda} = \\underset{w_0\\in\\mathbb{R}, w\\in\\mathbb{R}^d}{\\text{argmin}} \\Vert w_0 + Xw - y \\Vert^2_2 +\n",
    "    \\lambda \\Vert w \\Vert^2_2\n",
    "$$\n",
    "$$\n",
    "\\hat w^{lasso}_{\\lambda} = \\underset{w_0\\in\\mathbb{R}, w\\in\\mathbb{R}^d}{\\text{argmin}} \\Vert w_0 + Xw - y \\Vert^2_2\\ +\n",
    "    \\lambda \\Vert w \\Vert_1\n",
    "$$\n",
    "\n",
    "Let us fit both models of Ridge and Lasso over the `mtcars` dataset for different values of the regularization parameter $\\lambda$. For each value of $\\lambda$ we will store the fitted coefficients as well as the losses they achieve (both the MSE and the regularization term values).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\noams\\anaconda3\\envs\\iml.env\\lib\\site-packages\\sklearn\\linear_model\\_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  FutureWarning,\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'Merc 230'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12980\\2032292813.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mlam\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlambdas\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"model\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[0mres\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"coefs\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12980\\2032292813.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(lam, x, y)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m models = [dict(name=\"Lasso\",\n\u001b[1;32m----> 4\u001b[1;33m                \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mlam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mLasso\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m                reg_penalty= lambda lam, w: lam*np.linalg.norm(w, ord=1)),\n\u001b[0;32m      6\u001b[0m          dict(name=\"Ridge\",\n",
      "\u001b[1;32m~\\anaconda3\\envs\\iml.env\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    941\u001b[0m                 \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX_copied\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    942\u001b[0m                 \u001b[0mmulti_output\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 943\u001b[1;33m                 \u001b[0my_numeric\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    944\u001b[0m             )\n\u001b[0;32m    945\u001b[0m             y = check_array(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\iml.env\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    579\u001b[0m                 \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    580\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 581\u001b[1;33m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    582\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    583\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\iml.env\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m    974\u001b[0m         \u001b[0mensure_min_samples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mensure_min_samples\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    975\u001b[0m         \u001b[0mensure_min_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mensure_min_features\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 976\u001b[1;33m         \u001b[0mestimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    977\u001b[0m     )\n\u001b[0;32m    978\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\iml.env\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    744\u001b[0m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"unsafe\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    745\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 746\u001b[1;33m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    747\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    748\u001b[0m                 raise ValueError(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\iml.env\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m   1991\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1992\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mNpDtype\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1993\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1994\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1995\u001b[0m     def __array_wrap__(\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'Merc 230'"
     ]
    }
   ],
   "source": [
    "lambdas = 10**np.linspace(-3, 2, 100)\n",
    "\n",
    "models = [dict(name=\"Lasso\",\n",
    "               model=lambda lam, x, y: Lasso(alpha=lam, normalize=True, max_iter=10000, tol=1e-4).fit(x, y),\n",
    "               reg_penalty= lambda lam, w: lam*np.linalg.norm(w, ord=1)),\n",
    "         dict(name=\"Ridge\",\n",
    "               model=lambda lam, x, y: Ridge(alpha=lam, normalize=True).fit(x, y),\n",
    "               reg_penalty= lambda lam, w: lam*np.linalg.norm(w, ord=2))]\n",
    "\n",
    "\n",
    "regressors = {}\n",
    "for m in models:\n",
    "    res = dict(coefs  = pd.DataFrame([], columns=list(X_train.columns),  index = lambdas),\n",
    "               losses = pd.DataFrame([], columns=[\"mse\", \"reg\", \"loss\"], index = lambdas))\n",
    "   \n",
    "    for lam in lambdas:\n",
    "        model = m[\"model\"](lam, X_train, y_train)\n",
    "        res[\"coefs\"].loc[lam, :] = model.coef_\n",
    "        \n",
    "        mse = mean_squared_error(y_test, model.predict(X_test))\n",
    "        reg = m[\"reg_penalty\"](lam, model.coef_)\n",
    "        res[\"losses\"].loc[lam, :] = [mse, reg, mse+reg]\n",
    "        \n",
    "    regressors[m[\"name\"]] = res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Results\n",
    "\n",
    "Beginning with the Ridge regression, let us look at the losses and regularization path for each value of $\\lambda$. For each such value of $\\lambda$ we got a different model that minimized the joint loss function. We can see that as we increase $\\lambda$ all coefficients go towards $0$. This is known as shrinkage of the coefficients. \n",
    "\n",
    "For $\\lambda=100$, the penalty induced by the regularization term is so large that all coefficients are very close to zero, though for this dataset non is actually getting zero. Looking at the losses graph we see that the $\\lambda$ achieving lowest joint loss (MSE and regularization together) over the test set is $\\lambda\\approx0.1047$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs, losses = regressors[\"Ridge\"].values()\n",
    "\n",
    "\n",
    "fig = make_subplots(rows=2, cols=1, subplot_titles=[r'$\\text{Regularization Path}$', r'$\\text{Model Losses}$'], \n",
    "                    row_heights=[400,200], vertical_spacing=.1)\n",
    "\n",
    "# Plot the regularization path for each feature\n",
    "for i, col in enumerate(X_train.columns):\n",
    "    fig.add_trace(go.Scatter(x=lambdas, y=coefs.loc[:, col], mode='lines', name=col, legendgroup=\"1\"))\n",
    "\n",
    "\n",
    "# Plot the losses graph and mark lambda with lowest loss\n",
    "lam = np.argmin(losses.loc[:, 'loss'])\n",
    "fig.add_traces([go.Scatter(x=lambdas, y=losses.loc[:, 'mse'], mode='lines', name=\"Fidelity Term - MSE\", legendgroup=\"2\"),\n",
    "                go.Scatter(x=lambdas, y=losses.loc[:, 'reg'], mode='lines', name=\"Regularization Term\", legendgroup=\"2\"),\n",
    "                go.Scatter(x=lambdas, y=losses.loc[:, 'loss'], mode='lines', name=\"Joint Loss\", legendgroup=\"2\"),\n",
    "                go.Scatter(x=[lambdas[lam]], y=[losses.loc[:, 'loss'].values[lam]], mode='markers', showlegend=False,\n",
    "                           marker=dict(size=8, symbol=\"x\"), hovertemplate=\"Lambda: %{x}<extra></extra>\")], 2, 1)\n",
    "\n",
    "fig.update_layout(hovermode='x unified', margin=dict(t=50), \n",
    "                  legend=dict(tracegroupgap = 60),\n",
    "                  title=r\"$(1)\\text{ Fitting Ridge Regression}$\")\\\n",
    "   .update_xaxes(type=\"log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we look at the results of fitting a Lasso regression. Recall that the \"only\" difference between these two optimization problems is that Lasso uses the $\\ell_1$ norm as the regularization term. By doing so it introduces **sparsity** to the solutions. Similar to the Ridge regression coefficients are shrinked towards zero, but due to the sparsity, we are more likely to get coefficients of *exactly* zero.\n",
    "\n",
    "For example:\n",
    "- For $\\lambda=0.00722$ we observe the `cyl` feature getting a zero coefficient.\n",
    "- For $\\lambda=0.0327$ we observe the `drat` feature getting a zero coefficient.\n",
    "- For $\\lambda=1.353$ we observe the `wt` feature getting a zero coefficient, which at this point all features are fitted with a coefficient of zero.\n",
    "\n",
    "The regularization parameter that yields the lowest joint loss over the test set is $\\lambda=0.2983$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs, losses = regressors[\"Lasso\"].values()\n",
    "\n",
    "\n",
    "fig = make_subplots(rows=2, cols=1, subplot_titles=[r'$\\text{Regularization Path}$', r'$\\text{Model Losses}$'], \n",
    "                    row_heights=[400,200], vertical_spacing=.1)\n",
    "\n",
    "# Plot the regularization path for each feature\n",
    "for i, col in enumerate(X_train.columns):\n",
    "    fig.add_trace(go.Scatter(x=lambdas, y=coefs.loc[:, col], mode='lines', name=col, legendgroup=\"1\"))\n",
    "\n",
    "\n",
    "# Plot the losses graph and mark lambda with lowest loss\n",
    "lam = np.argmin(losses.loc[:, 'loss'])\n",
    "fig.add_traces([go.Scatter(x=lambdas, y=losses.loc[:, 'mse'], mode='lines', name=\"Fidelity Term - MSE\", legendgroup=\"2\"),\n",
    "                go.Scatter(x=lambdas, y=losses.loc[:, 'reg'], mode='lines', name=\"Regularization Term\", legendgroup=\"2\"),\n",
    "                go.Scatter(x=lambdas, y=losses.loc[:, 'loss'], mode='lines', name=\"Joint Loss\", legendgroup=\"2\"),\n",
    "                go.Scatter(x=[lambdas[lam]], y=[losses.loc[:, 'loss'].values[lam]], mode='markers', showlegend=False,\n",
    "                           marker=dict(size=8, symbol=\"x\"), hovertemplate=\"Lambda: %{x}<extra></extra>\")], 2, 1)\n",
    "\n",
    "fig.update_layout(hovermode='x unified', margin=dict(t=50), \n",
    "                  legend=dict(tracegroupgap = 60),\n",
    "                  title=r\"$(2)\\text{ Fitting Lasso Regression}$\")\\\n",
    "   .update_xaxes(type=\"log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Shrinkage of Coefficients\n",
    "Let us compare between the fitted coefficients of the Least Squares (LS), the Lasso and the Ridge regressions. For the Lasso and Ridge regressions we will select the $\\lambda$ achieving the lowest test error.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = LinearRegression().fit(X_train, y_train).coef_\n",
    "ridge = regressors[\"Ridge\"][\"coefs\"].iloc[np.argmin(regressors[\"Lasso\"][\"losses\"].loc[:, \"loss\"])]\n",
    "lasso = regressors[\"Lasso\"][\"coefs\"].iloc[np.argmin(regressors[\"Lasso\"][\"losses\"].loc[:, \"loss\"])]\n",
    "\n",
    "coefs = np.array([ls, ridge, lasso])\n",
    "pd.DataFrame(coefs, columns=list(X_train.columns), index=[\"LS\", \"Ridge\", \"Lasso\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following visualization of the coefficients under different models we can see the shrinkage of the coefficients. Notice that between the LS and Ridge models the overall size of the coefficients decreases. A similar outcome is seen between the LS and Lasso coefficients, with the difference that in the Lasso case many features are given a $0$ coefficient. In fact, only the `vs`, `cyl` and `wt` features are with non-zero coefficients. This shows how the Lasso regression also performs a type of feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure(layout=go.Layout(title=r\"$\\text{(3) Regression Models Coefficients}$\",\n",
    "                                 xaxis=dict(range=[-1.1, 1.1], showticklabels=False, zeroline=False) ))\n",
    "\n",
    "fig.add_annotation(x=-.8, y=6, text=r\"$\\text{LS Coefficients}$\",   showarrow=False)\n",
    "fig.add_annotation(x=0,   y=6, text=r\"$\\text{Ridge Coefficients}$\", showarrow=False)\n",
    "fig.add_annotation(x=.8,  y=6, text=r\"$\\text{Lasso Coefficients}$\", showarrow=False)\n",
    "\n",
    "for i, col in enumerate(X_train.columns):\n",
    "    fig.add_trace(go.Scatter(x=[-1, 0, 1], y=coefs[:, i], mode='markers+lines', name=col, \n",
    "                             line={'dash':'dot'}))\n",
    "\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
